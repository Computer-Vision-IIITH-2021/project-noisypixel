{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "closed-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import skimage.io as sio\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heated-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/shubham/datasets/subset/ShapeNet/\"\n",
    "dataset_dir = \"/home/shubham/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "novel-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(dataset_dir, \"hdf_data\"), exist_ok=True)\n",
    "save_path = os.path.join(dataset_dir, \"hdf_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(dic, filename):\n",
    "    \"\"\"\n",
    "    ....\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'w') as h5file:\n",
    "        recursively_save_dict_contents_to_group(h5file, '/', dic)\n",
    "\n",
    "def recursively_save_dict_contents_to_group(h5file, path, dic):\n",
    "    \"\"\"\n",
    "    ....\n",
    "    \"\"\"\n",
    "    for key, item in dic.items():\n",
    "        if isinstance(item, (np.ndarray, np.int64, np.float64, str, bytes)):\n",
    "            h5file[path + key] = item\n",
    "        elif isinstance(item, dict):\n",
    "            recursively_save_dict_contents_to_group(h5file, path + key + '/', item)\n",
    "        else:\n",
    "            raise ValueError('Cannot save %s type'%type(item))\n",
    "\n",
    "def load_dict_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    ....\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        return recursively_load_dict_contents_from_group(h5file, '/')\n",
    "\n",
    "def recursively_load_dict_contents_from_group(h5file, path):\n",
    "    \"\"\"\n",
    "    ....\n",
    "    \"\"\"\n",
    "    ans = {}\n",
    "    for key, item in h5file[path].items():\n",
    "        if isinstance(item, h5py._hl.dataset.Dataset):\n",
    "            ans[key] = item.value\n",
    "        elif isinstance(item, h5py._hl.group.Group):\n",
    "            ans[key] = recursively_load_dict_contents_from_group(h5file, path + key + '/')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):   \n",
    "    # Load the pointcloud.npz and points.npz file\n",
    "    pc_file = np.load(os.path.join(path, \"pointcloud.npz\"))\n",
    "    points_file = np.load(os.path.join(path, \"points.npz\"))\n",
    "    \n",
    "    # create image placeholder and camera data placeholder\n",
    "    img_data = []\n",
    "    cam_data = None\n",
    "    \n",
    "    # Load images\n",
    "    for imx in os.listdir(os.path.join(path, \"img_choy2016\")):\n",
    "        current = os.path.join(path, \"img_choy2016\", imx)\n",
    "        if 'npz' in imx:\n",
    "            cam_data = np.load(current)\n",
    "        else:\n",
    "            img_current = sio.imread(current)\n",
    "            if img_current.ndim == 2:\n",
    "                img_current = np.stack([img_current, img_current, img_current], axis=-1)\n",
    "            img_data.append(img_current)\n",
    "    img_data = np.asarray(img_data)\n",
    "    \n",
    "    all_data = {\n",
    "        'images': img_data,\n",
    "        'camera': dict(cam_data),\n",
    "        'points': dict(points_file),\n",
    "        'pointcloud': dict(pc_file)\n",
    "    }\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-brazil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-uncle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "velvet-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shubham/datasets/hdf_data/02933112_7238faf31667078b2ea98d69e91ba870.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_1f674f735abb7b1d75869f989849123f.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_6219b46946f62474c62bee40dcdc539.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_346419b6e2131dda5785f58f071c8c43.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_1175801334a9e410df3a1b0d597ce76e.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_6352c69907b70c0480fa521a9c7198a.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_809d5384f84d55273a11565e5be9cf53.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_12f1e4964078850cc7113d9e058b9db7.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_8fd43ffcc981f6eb14038d588fd1342f.h5\n",
      "/home/shubham/datasets/hdf_data/02933112_4c8e95fe5fdbb125c59350d819542ec7.h5\n"
     ]
    }
   ],
   "source": [
    "for cid in os.listdir(data_root)[:1]:\n",
    "    objs_path = os.path.join(data_root, cid)\n",
    "    obj_list = os.listdir(objs_path)\n",
    "    for obx in obj_list[:10]:\n",
    "        current_path = os.path.join(objs_path, obx)\n",
    "        new_filename = \"{}_{}.h5\".format(cid, obx)\n",
    "        \n",
    "        try:\n",
    "            data_current = load_data(current_path)\n",
    "            save_dict_to_hdf5(data_current, os.path.join(save_path, new_filename))\n",
    "        except:\n",
    "            print(\"Error at {}-{}\".format(cid, obx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "original-simulation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "presidential-cooling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-examination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
