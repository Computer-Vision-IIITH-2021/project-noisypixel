{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assured-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "closing-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyNetDatasetHDF(Dataset):\n",
    "    \"\"\"Occupancy Network dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, num_points=1024):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            num_points (int): Number of points to sample in the object point cloud from the data\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_points = num_points\n",
    "        self.files = []\n",
    "        \n",
    "        for sub in os.listdir(self.root_dir):\n",
    "            self.files.append(sub)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the file path and setup image folder paths\n",
    "        req_path = self.files[idx]\n",
    "        file_path = os.path.join(self.root_dir, req_path)\n",
    "\n",
    "        # Load the h5 file\n",
    "        hf = h5py.File(file_path, 'r')\n",
    "        \n",
    "        # [NOTE]: the notation [()] below is to extract the value from HDF5 file\n",
    "        # get all images and randomly pick one\n",
    "        all_imgs = hf['images'][()]\n",
    "        random_idx = int(np.random.random()*all_imgs.shape[0])\n",
    "        \n",
    "        # Fetch the image we need\n",
    "        image = all_imgs[random_idx]\n",
    "        \n",
    "        # Get the points and occupancies\n",
    "        points = hf['points']['points'][()]\n",
    "        occupancies = np.unpackbits(hf['points']['occupancies'][()])\n",
    "\n",
    "        # Sample n points from the data\n",
    "        selected_idx = np.random.permutation(np.arange(points.shape[0]))[:self.num_points]\n",
    "\n",
    "        # Use only the selected indices and pack everything up in a nice dictionary\n",
    "        sample = (\n",
    "          torch.from_numpy(image).float().transpose(1, 2).transpose(0, 1), \n",
    "          torch.from_numpy(points[selected_idx]), \n",
    "          torch.from_numpy(occupancies[selected_idx]))\n",
    "        \n",
    "        # Close the hdf file\n",
    "        hf.close()\n",
    "        \n",
    "        # Apply any transformation necessary\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "congressional-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = OccupancyNetDatasetHDF(\"/home/shubham/datasets/hdf_data/\", num_points=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "overhead-tension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True)\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "official-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([32, 3, 137, 137]) torch.Size([32, 1024, 3]) torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "for ix in loader:\n",
    "    print(ix[0].shape, ix[1].shape, ix[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-snake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
