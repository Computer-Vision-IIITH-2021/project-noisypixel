{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "after-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "charitable-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "middle-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyNetDatasetHDF(Dataset):\n",
    "    \"\"\"Occupancy Network dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, num_points=1024, default_transform=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            num_points (int): Number of points to sample in the object point cloud from the data\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_points = num_points\n",
    "        self.files = []\n",
    "        \n",
    "        for sub in os.listdir(self.root_dir):\n",
    "            self.files.append(sub)\n",
    "            \n",
    "        # If not transforms have been provided, apply default imagenet transform\n",
    "        if transform is None and default_transform:\n",
    "            self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the file path and setup image folder paths\n",
    "        req_path = self.files[idx]\n",
    "        file_path = os.path.join(self.root_dir, req_path)\n",
    "\n",
    "        # Load the h5 file\n",
    "        hf = h5py.File(file_path, 'r')\n",
    "        \n",
    "        # [NOTE]: the notation [()] below is to extract the value from HDF5 file\n",
    "        # get all images and randomly pick one\n",
    "        all_imgs = hf['images'][()]\n",
    "        random_idx = int(np.random.random()*all_imgs.shape[0])\n",
    "        \n",
    "        # Fetch the image we need\n",
    "        image = all_imgs[random_idx]\n",
    "        \n",
    "        # Get the points and occupancies\n",
    "        points = hf['points']['points'][()]\n",
    "        occupancies = np.unpackbits(hf['points']['occupancies'][()])\n",
    "\n",
    "        # Sample n points from the data\n",
    "        selected_idx = np.random.permutation(np.arange(points.shape[0]))[:self.num_points]\n",
    "\n",
    "        # Use only the selected indices and pack everything up in a nice dictionary\n",
    "        final_image = torch.from_numpy(image).float().transpose(1, 2).transpose(0, 1)\n",
    "        final_points = torch.from_numpy(points[selected_idx])\n",
    "        final_gt = torch.from_numpy(occupancies[selected_idx])\n",
    "        \n",
    "        # Close the hdf file\n",
    "        hf.close()\n",
    "        \n",
    "        # Apply any transformation necessary\n",
    "        if self.transform:\n",
    "            final_image = self.transform(final_image)\n",
    "\n",
    "        return final_image, final_points, final_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-participation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "recognized-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = OccupancyNetDatasetHDF(\"/home/shubham/datasets/hdf_data/\", num_points=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "senior-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1051.4952)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hungry-defendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True)\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dedicated-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([128, 3, 137, 137]) torch.Size([128, 1024, 3]) torch.Size([128, 1024])\n",
      "torch.Size([32, 3, 137, 137]) torch.Size([32, 1024, 3]) torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "for ix in loader:\n",
    "    print(ix[0].shape, ix[1].shape, ix[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "collective-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "net = EfficientNet.from_pretrained('efficientnet-b7', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-police",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "southwest-actor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2560, 1, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(ds[0][0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-dominant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
