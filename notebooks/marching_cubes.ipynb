{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from im2mesh.utils.libmise.mise import  MISE\n",
    "from im2mesh.utils.libmcubes.mcubes import marching_cubes\n",
    "import trimesh\n",
    "import os\n",
    "\n",
    "#model\n",
    "from im2mesh import config\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from im2mesh.checkpoints import CheckpointIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config.load_config( 'configs/demo.yaml', 'configs/default.yaml') \n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.eu-central-1.amazonaws.com/avg-projects/occupancy_networks/models/onet_img2mesh_3-f786b04a.pt\n",
      "=> Loading checkpoint from url...\n"
     ]
    }
   ],
   "source": [
    "out_dir = cfg['training']['out_dir']\n",
    "generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])\n",
    "out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')\n",
    "out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')\n",
    "\n",
    "batch_size = cfg['generation']['batch_size']\n",
    "input_type = cfg['data']['input_type']\n",
    "vis_n_outputs = cfg['generation']['vis_n_outputs']\n",
    "if vis_n_outputs is None:\n",
    "    vis_n_outputs = -1\n",
    "\n",
    "# Dataset\n",
    "dataset = config.get_dataset('test', cfg, return_idx=True)\n",
    "\n",
    " \n",
    "model = config.get_model(cfg, device=device, dataset=dataset)\n",
    "\n",
    "checkpoint_io = CheckpointIO(out_dir, model=model)\n",
    "checkpoint_io.load(cfg['test']['model_file'])\n",
    "\n",
    "# Generator\n",
    "generator = config.get_generator(model, cfg, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OccupancyNetwork(\n",
       "  (decoder): DecoderCBatchNorm(\n",
       "    (fc_p): Conv1d(3, 256, kernel_size=(1,), stride=(1,))\n",
       "    (block0): CResnetBlockConv1d(\n",
       "      (bn_0): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (bn_1): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (actvn): ReLU()\n",
       "    )\n",
       "    (block1): CResnetBlockConv1d(\n",
       "      (bn_0): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (bn_1): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (actvn): ReLU()\n",
       "    )\n",
       "    (block2): CResnetBlockConv1d(\n",
       "      (bn_0): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (bn_1): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (actvn): ReLU()\n",
       "    )\n",
       "    (block3): CResnetBlockConv1d(\n",
       "      (bn_0): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (bn_1): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (actvn): ReLU()\n",
       "    )\n",
       "    (block4): CResnetBlockConv1d(\n",
       "      (bn_0): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (bn_1): CBatchNorm1d(\n",
       "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      )\n",
       "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (actvn): ReLU()\n",
       "    )\n",
       "    (bn): CBatchNorm1d(\n",
       "      (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    )\n",
       "    (fc_out): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (encoder): Resnet18(\n",
       "    (features): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "      (fc): Sequential()\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader( dataset, batch_size=1, num_workers=0, shuffle=False)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 0.1\n",
    "threshold_g = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3d_grid(bb_min, bb_max, shape):\n",
    "    ''' Makes a 3D grid.\n",
    "\n",
    "    Args:\n",
    "        bb_min (tuple): bounding box minimum\n",
    "        bb_max (tuple): bounding box maximum\n",
    "        shape (tuple): output shape\n",
    "    '''\n",
    "    size = shape[0] * shape[1] * shape[2]\n",
    "\n",
    "    pxs = torch.linspace(bb_min[0], bb_max[0], shape[0])\n",
    "    pys = torch.linspace(bb_min[1], bb_max[1], shape[1])\n",
    "    pzs = torch.linspace(bb_min[2], bb_max[2], shape[2])\n",
    "\n",
    "    pxs = pxs.view(-1, 1, 1).expand(*shape).contiguous().view(size)\n",
    "    pys = pys.view(1, -1, 1).expand(*shape).contiguous().view(size)\n",
    "    pzs = pzs.view(1, 1, -1).expand(*shape).contiguous().view(size)\n",
    "    p = torch.stack([pxs, pys, pzs], dim=1)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mesh(occ_hat):\n",
    "    n_x, n_y, n_z = occ_hat.shape\n",
    "    box_size = 1 + padding\n",
    "    threshold = np.log( threshold_g) - np.log(1. - threshold_g)\n",
    "    \n",
    "    occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)\n",
    "    \n",
    "    vertices, triangles = marching_cubes(occ_hat_padded, threshold)\n",
    "  \n",
    "    vertices -= 0.5\n",
    "    # Undo padding\n",
    "    vertices -= 1\n",
    "    # Normalize to bounding box\n",
    "    vertices /= np.array([n_x-1, n_y-1, n_z-1])\n",
    "    vertices = box_size * (vertices - 0.5)\n",
    "    \n",
    "    normals = None\n",
    "\n",
    "    # Create mesh\n",
    "    mesh = trimesh.Trimesh(vertices, triangles, vertex_normals=normals,process=False)\n",
    "\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_points(p, z, c=None  ):\n",
    "    points_batch_size=100000\n",
    "    p_split = torch.split(p, points_batch_size)\n",
    "    occ_hats = []\n",
    "\n",
    "    for pi in p_split:\n",
    "        pi = pi.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            occ_hat = model.decode(pi,z,c).logits\n",
    "\n",
    "        occ_hats.append(occ_hat.squeeze(0).detach().cpu())\n",
    "\n",
    "    occ_hat = torch.cat(occ_hats, dim=0)\n",
    "\n",
    "    return occ_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh(occ, points,threshold = 0.5,padding=0.1,resolution0=32,upsampling_steps=2):\n",
    "    model.eval()\n",
    "    threshold = np.log(threshold_g) - np.log(1. - threshold_g)\n",
    "    \n",
    "    nx = 32\n",
    "    pointsf = 2 * make_3d_grid((-0.5,)*3, (0.5,)*3, (nx,)*3    )\n",
    "    for i,data in enumerate(tqdm(test_loader)):\n",
    "        test_sample = data\n",
    "        break\n",
    "    \n",
    "    inputs = test_sample.get('inputs', torch.empty(1, 0)).to(device)\n",
    "    \n",
    "#     points = mesh_extractor.query()\n",
    "#     value_grid = occ.reshape(nx, nx, nx)\n",
    "    z = model.get_z_from_prior((1,), sample=False).to(device)\n",
    "    c = model.encode_inputs(inputs)\n",
    "    values = eval_points(pointsf, z,c ).cpu().numpy()\n",
    "    value_grid = values.reshape(nx, nx, nx)\n",
    "     \n",
    "    mesh = extract_mesh(value_grid )\n",
    "\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OFF\\n470 936 0\\n-0.2666191939 -0.0532258065 -0.0887096774\\n-0.2661290323 -0.0573409627 -0.0887096774\\n-0.2661290323 -0.0532258065 -0.0929659685\\n-0.2673791842 -0.0532258065 -0.0532258065\\n-0.2661290323 -0.0636678603 -0.0532258065\\n-0.2675825161 -0.0532258065 -0.0177419355\\n-0.2661290323 -0.0656581460 -0.0177419355\\n-0.2675704905 -0.0532258065 0.0177419355\\n-0.2661290323 -0.0653436481 0.0177419355\\n-0.2674851017 -0.0532258065 0.0532258065\\n-0.2661290323 -0.0643696959 0.0532258065\\n-0.2668755606 -0.0532258065 0.0887096774\\n-0.2661290323 -0.0590907636 0.0887096774\\n-0.2661290323 -0.0532258065 0.0967099254\\n-0.2661290323 -0.0425152834 -0.0887096774\\n-0.2661290323 -0.0227933788 -0.0532258065\\n-0.2661409607 -0.0177419355 -0.0177419355\\n-0.2661290323 -0.0177419355 -0.0195624129\\n-0.2661290323 -0.0190353710 0.0177419355\\n-0.2661290323 -0.0177419355 -0.0115374126\\n-0.2661290323 -0.0235377009 0.0532258065\\n-0.2661290323 -0.0371818080 0.0887096774\\n-0.2661290323 -0.0170838481 -0.0177419355\\n-0.2306451613 -0.0734543449 -0.0887096774\\n-0.2306451613 -0.0532258065 -0.1004418050\\n-0.2306451613 -0.0605269558 -0.0532258065\\n-0.2306451613 -0.0556767116 -0.0177419355\\n-0.2306451613 -0.0557182164 0.0177419355\\n-0.2306451613 -0.0610457334 0.0532258065\\n-0.2306451613 -0.0733941465 0.0887096774\\n-0.2306451613 -0.0532258065 0.1013960051\\n-0.2580636452 -0.0177419355 -0.0887096774\\n-0.2306451613 -0.0177419355 -0.1095756605\\n-0.2643657424 -0.0177419355 -0.0532258065\\n-0.2656673621 -0.0177419355 0.0177419355\\n-0.2639100349 -0.0177419355 0.0532258065\\n-0.2590540938 -0.0177419355 0.0887096774\\n-0.2306451613 -0.0177419355 0.1080841006\\n-0.2555373219 0.0177419355 -0.0887096774\\n-0.2306451613 0.0177419355 -0.1087065545\\n-0.2598866531 0.0177419355 -0.0532258065\\n-0.2612096805 0.0177419355 -0.0177419355\\n-0.2606038086 0.0177419355 0.0177419355\\n-0.2595999950 0.0177419355 0.0532258065\\n-0.2570077030 0.0177419355 0.0887096774\\n-0.2306451613 0.0177419355 0.1068708861\\n-0.2525311495 0.0532258065 -0.0887096774\\n-0.2306451613 0.0532258065 -0.1073177789\\n-0.2567136520 0.0532258065 -0.0532258065\\n-0.2584710592 0.0532258065 -0.0177419355\\n-0.2579097642 0.0532258065 0.0177419355\\n-0.2563104767 0.0532258065 0.0532258065\\n-0.2514324863 0.0532258065 0.0887096774\\n-0.2306451613 0.0532258065 0.1061127753\\n-0.2461525886 0.0887096774 -0.0887096774\\n-0.2306451613 0.0887096774 -0.1029167770\\n-0.2499962396 0.0887096774 -0.0532258065\\n-0.2492103015 0.0887096774 -0.0177419355\\n-0.2492472909 0.0887096774 0.0177419355\\n-0.2499725764 0.0887096774 0.0532258065\\n-0.2464499672 0.0887096774 0.0887096774\\n-0.2306451613 0.0887096774 0.1010532658\\n-0.2306451613 0.1095960391 -0.0887096774\\n-0.2306451613 0.1120725386 -0.0532258065\\n-0.2306451613 0.1120970110 -0.0177419355\\n-0.2306451613 0.1122274933 0.0177419355\\n-0.2306451613 0.1130803196 0.0532258065\\n-0.2306451613 0.1097968764 0.0887096774\\n-0.2164944487 -0.0887096774 -0.0887096774\\n-0.1951612903 -0.1189581504 -0.0887096774\\n-0.1951612903 -0.0887096774 -0.1023005382\\n-0.1951612903 -0.0887096774 -0.0660680849\\n-0.2170267184 -0.0887096774 0.0887096774\\n-0.1951612903 -0.1223745045 0.0887096774\\n-0.1951612903 -0.0887096774 0.0656395499\\n-0.1951612903 -0.0887096774 0.1017712112\\n-0.1951612903 -0.0532258065 -0.1030971149\\n-0.1951612903 -0.0546069401 -0.0532258065\\n-0.2203619734 -0.0532258065 -0.0177419355\\n-0.1951612903 -0.0532258065 -0.0503221170\\n-0.2210893771 -0.0532258065 0.0177419355\\n-0.1967487881 -0.0532258065 0.0532258065\\n-0.1951612903 -0.0532258065 0.0536349398\\n-0.1951612903 -0.0532258065 0.1038807367\\n-0.1951612903 -0.0177419355 -0.0978457889\\n-0.1951612903 -0.0497666972 -0.0177419355\\n-0.1951612903 -0.0493676546 0.0177419355\\n-0.1951612903 -0.0529887649 0.0532258065\\n-0.1951612903 -0.0177419355 0.1007467575\\n-0.1951612903 0.0177419355 -0.1089369813\\n-0.1951612903 0.0177419355 0.1076041935\\n-0.1951612903 0.0532258065 -0.1092003626\\n-0.1951612903 0.0532258065 0.1075612826\\n-0.1951612903 0.0887096774 -0.1006790724\\n-0.1951612903 0.0887096774 0.1004756617\\n-0.1951612903 0.1070845079 -0.0887096774\\n-0.1951612903 0.1128875520 -0.0532258065\\n-0.1951612903 0.1143082853 -0.0177419355\\n-0.1951612903 0.1141765049 0.0177419355\\n-0.1951612903 0.1133426050 0.0532258065\\n-0.1951612903 0.1080129575 0.0887096774\\n-0.1881331762 -0.1241935484 -0.0887096774\\n-0.1596774194 -0.1304681920 -0.0887096774\\n-0.1596774194 -0.1241935484 -0.0978768832\\n-0.1596774194 -0.1241935484 -0.0783223027\\n-0.1929261474 -0.1241935484 0.0887096774\\n-0.1596774194 -0.1309512981 0.0887096774\\n-0.1596774194 -0.1241935484 0.0764916186\\n-0.1596774194 -0.1241935484 0.1017824819\\n-0.1596774194 -0.0887096774 -0.0972283152\\n-0.1596774194 -0.0887096774 -0.0577313202\\n-0.1736428197 -0.0887096774 -0.0177419355\\n-0.1596774194 -0.0908607378 -0.0177419355\\n-0.1596774194 -0.0887096774 -0.0434151241\\n-0.1719809609 -0.0887096774 0.0177419355\\n-0.1596774194 -0.0906658819 0.0177419355\\n-0.1596774194 -0.0887096774 0.0391747033\\n-0.1596774194 -0.0887096774 0.0593562113\\n-0.1596774194 -0.0887096774 0.0971661265\\n-0.1596774194 -0.0532258065 -0.1011245645\\n-0.1596774194 -0.0812325284 -0.0532258065\\n-0.1691844438 -0.0532258065 -0.0177419355\\n-0.1676410465 -0.0532258065 0.0177419355\\n-0.1932875378 -0.0532258065 0.0532258065\\n-0.1596774194 -0.0771179080 0.0532258065\\n-0.1596774194 -0.0532258065 0.1022607872\\n-0.1596774194 -0.0177419355 -0.0965021011\\n-0.1596774194 -0.0177419355 0.0980610714\\n-0.1596774194 0.0177419355 -0.1083206341\\n-0.1596774194 0.0177419355 0.1071158214\\n-0.1596774194 0.0532258065 -0.1092298082\\n-0.1596774194 0.0532258065 0.1065829875\\n-0.1596774194 0.0887096774 -0.1013606649\\n-0.1596774194 0.0887096774 0.1006374975\\n-0.1596774194 0.1079676998 -0.0887096774\\n-0.1596774194 0.1112911192 -0.0532258065\\n-0.1596774194 0.1120868909 -0.0177419355\\n-0.1596774194 0.1119533135 0.0177419355\\n-0.1596774194 0.1119510570 0.0532258065\\n-0.1596774194 0.1088165307 0.0887096774\\n-0.1491584708 -0.1241935484 -0.0887096774\\n-0.1489489874 -0.1241935484 0.0887096774\\n-0.1241935484 -0.1057045719 -0.0887096774\\n-0.1241935484 -0.0887096774 -0.1106623037\\n-0.1241935484 -0.0887096774 -0.0654902993\\n-0.1521745742 -0.0887096774 -0.0177419355\\n-0.1529231055 -0.0887096774 0.0177419355\\n-0.1241935484 -0.1043707961 0.0887096774\\n-0.1241935484 -0.0887096774 0.0656970284\\n-0.1241935484 -0.0887096774 0.1091370899\\n-0.1241935484 -0.0532258065 -0.1077176317\\n-0.1241935484 -0.0629286065 -0.0532258065\\n-0.1443744509 -0.0532258065 -0.0177419355\\n-0.1241935484 -0.0532258065 -0.0280479463\\n-0.1435444217 -0.0532258065 0.0177419355\\n-0.1241935484 -0.0627968558 0.0532258065\\n-0.1241935484 -0.0532258065 0.0265547091\\n-0.1241935484 -0.0532258065 0.1088645324\\n-0.1241935484 -0.0177419355 -0.0977902473\\n-0.1241935484 -0.0515332947 -0.0177419355\\n-0.1241935484 -0.0518390182 0.0177419355\\n-0.1241935484 -0.0177419355 0.1001634554\\n-0.1241935484 0.0177419355 -0.1069368929\\n-0.1241935484 0.0177419355 0.1071360554\\n-0.1241935484 0.0532258065 -0.1077976487\\n-0.1241935484 0.0532258065 0.1055825039\\n-0.1241935484 0.0887096774 -0.1009351823\\n-0.1241935484 0.0887096774 0.1008769791\\n-0.1241935484 0.1087564334 -0.0887096774\\n-0.1241935484 0.1102241642 -0.0532258065\\n-0.1241935484 0.1114601178 -0.0177419355\\n-0.1241935484 0.1114613914 0.0177419355\\n-0.1241935484 0.1109004370 0.0532258065\\n-0.1241935484 0.1099003549 0.0887096774\\n-0.1058509725 -0.0887096774 -0.0887096774\\n-0.1060062818 -0.0887096774 0.0887096774\\n-0.0887096774 -0.0729811086 -0.0887096774\\n-0.0887096774 -0.0532258065 -0.1026016360\\n-0.0887096774 -0.0696283982 -0.0532258065\\n-0.1204424248 -0.0532258065 -0.0177419355\\n-0.0887096774 -0.0680222452 -0.0177419355\\n-0.1211229374 -0.0532258065 0.0177419355\\n-0.0887096774 -0.0681982064 0.0177419355\\n-0.0887096774 -0.0697653688 0.0532258065\\n-0.0887096774 -0.0736122541 0.0887096774\\n-0.0887096774 -0.0532258065 0.1071506970\\n-0.0887096774 -0.0177419355 -0.1057964720\\n-0.0887096774 -0.0177419355 0.1072977713\\n-0.0887096774 0.0177419355 -0.1058308648\\n-0.0887096774 0.0177419355 0.1070107616\\n-0.0887096774 0.0532258065 -0.1063912364\\n-0.0887096774 0.0532258065 0.1054132144\\n-0.0887096774 0.0887096774 -0.1008009796\\n-0.0887096774 0.0887096774 0.1005267454\\n-0.0887096774 0.1103254809 -0.0887096774\\n-0.0887096774 0.1131413710 -0.0532258065\\n-0.0887096774 0.1144354229 -0.0177419355\\n-0.0887096774 0.1142980506 0.0177419355\\n-0.0887096774 0.1135612601 0.0532258065\\n-0.0887096774 0.1098007418 0.0887096774\\n-0.0532258065 -0.0725310751 -0.0887096774\\n-0.0532258065 -0.0532258065 -0.1027483787\\n-0.0532258065 -0.0718927081 -0.0532258065\\n-0.0532258065 -0.0722192424 -0.0177419355\\n-0.0532258065 -0.0722085819 0.0177419355\\n-0.0532258065 -0.0713901978 0.0532258065\\n-0.0532258065 -0.0727087308 0.0887096774\\n-0.0532258065 -0.0532258065 0.1058288950\\n-0.0532258065 -0.0177419355 -0.1036749441\\n-0.0532258065 -0.0177419355 0.1058909386\\n-0.0532258065 0.0177419355 -0.1045660219\\n-0.0532258065 0.0177419355 0.1066227089\\n-0.0532258065 0.0532258065 -0.1062901877\\n-0.0532258065 0.0532258065 0.1057553895\\n-0.0532258065 0.0887096774 -0.1000490530\\n-0.0532258065 0.0887096774 0.0992390504\\n-0.0532258065 0.1120705294 -0.0887096774\\n-0.0532258065 0.1165641428 -0.0532258065\\n-0.0532258065 0.1174223464 -0.0177419355\\n-0.0532258065 0.1174239156 0.0177419355\\n-0.0532258065 0.1165152115 0.0532258065\\n-0.0532258065 0.1102947579 0.0887096774\\n-0.0177419355 -0.0720616176 -0.0887096774\\n-0.0177419355 -0.0532258065 -0.1031416928\\n-0.0177419355 -0.0717709160 -0.0532258065\\n-0.0177419355 -0.0718474116 -0.0177419355\\n-0.0177419355 -0.0718869038 0.0177419355\\n-0.0177419355 -0.0712529648 0.0532258065\\n-0.0177419355 -0.0721660655 0.0887096774\\n-0.0177419355 -0.0532258065 0.1060002584\\n-0.0177419355 -0.0177419355 -0.1037106105\\n-0.0177419355 -0.0177419355 0.1061030820\\n-0.0177419355 0.0177419355 -0.1040261104\\n-0.0177419355 0.0177419355 0.1066936717\\n-0.0177419355 0.0532258065 -0.1064347288\\n-0.0177419355 0.0532258065 0.1062813317\\n-0.0177419355 0.0887096774 -0.0993532923\\n-0.0177419355 0.0887096774 0.0982632447\\n-0.0177419355 0.1125314944 -0.0887096774\\n-0.0177419355 0.1184433435 -0.0532258065\\n-0.0177419355 0.1199154436 -0.0177419355\\n-0.0177419355 0.1196385968 0.0177419355\\n-0.0177419355 0.1180906210 0.0532258065\\n-0.0177419355 0.1098833949 0.0887096774\\n0.0177419355 -0.0718620247 -0.0887096774\\n0.0177419355 -0.0532258065 -0.1030798768\\n0.0177419355 -0.0717856793 -0.0532258065\\n0.0177419355 -0.0720064585 -0.0177419355\\n0.0177419355 -0.0719985268 0.0177419355\\n0.0177419355 -0.0713439101 0.0532258065\\n0.0177419355 -0.0721064290 0.0887096774\\n0.0177419355 -0.0532258065 0.1058124901\\n0.0177419355 -0.0177419355 -0.1038575495\\n0.0177419355 -0.0177419355 0.1057644394\\n0.0177419355 0.0177419355 -0.1041881860\\n0.0177419355 0.0177419355 0.1065284606\\n0.0177419355 0.0532258065 -0.1090320158\\n0.0177419355 0.0532258065 0.1068043588\\n0.0177419355 0.0887096774 -0.0990167914\\n0.0177419355 0.0887096774 0.0977100964\\n0.0177419355 0.1119790341 -0.0887096774\\n0.0177419355 0.1186773060 -0.0532258065\\n0.0177419355 0.1189968529 -0.0177419355\\n0.0177419355 0.1186405361 0.0177419355\\n0.0177419355 0.1172870197 0.0532258065\\n0.0177419355 0.1086444334 0.0887096774\\n0.0532258065 -0.0723354185 -0.0887096774\\n0.0532258065 -0.0532258065 -0.1032570834\\n0.0532258065 -0.0721409323 -0.0532258065\\n0.0532258065 -0.0716778618 -0.0177419355\\n0.0532258065 -0.0716673376 0.0177419355\\n0.0532258065 -0.0714877048 0.0532258065\\n0.0532258065 -0.0722965559 0.0887096774\\n0.0532258065 -0.0532258065 0.1054294284\\n0.0532258065 -0.0177419355 -0.1040860340\\n0.0532258065 -0.0177419355 0.1061958976\\n0.0532258065 0.0177419355 -0.1056102723\\n0.0532258065 0.0177419355 0.1077368490\\n0.0532258065 0.0532258065 -0.1118460289\\n0.0532258065 0.0532258065 0.1092182634\\n0.0532258065 0.0887096774 -0.0982146926\\n0.0532258065 0.0887096774 0.0963913298\\n0.0532258065 0.1094663735 -0.0887096774\\n0.0532258065 0.1158922147 -0.0532258065\\n0.0532258065 0.1148888513 -0.0177419355\\n0.0532258065 0.1148683176 0.0177419355\\n0.0532258065 0.1155005747 0.0532258065\\n0.0532258065 0.1053473767 0.0887096774\\n0.0887096774 -0.0732135319 -0.0887096774\\n0.0887096774 -0.0532258065 -0.1035587938\\n0.0887096774 -0.0727349085 -0.0532258065\\n0.0887096774 -0.0713489344 -0.0177419355\\n0.0887096774 -0.0713146209 0.0177419355\\n0.0887096774 -0.0721797784 0.0532258065\\n0.0887096774 -0.0729111796 0.0887096774\\n0.0887096774 -0.0532258065 0.1053463666\\n0.0887096774 -0.0177419355 -0.1050462679\\n0.0887096774 -0.0177419355 0.1080783512\\n0.0887096774 0.0177419355 -0.1125988445\\n0.0887096774 0.0177419355 0.1151802010\\n0.0887096774 0.0532258065 -0.1238409120\\n0.0821335296 0.0532258065 0.1241935484\\n0.0887096774 0.0445406237 0.1241935484\\n0.0887096774 0.0532258065 0.1279604503\\n0.0887096774 0.0887096774 -0.0936604986\\n0.0887096774 0.0570570763 0.1241935484\\n0.0887096774 0.0887096774 0.0913714442\\n0.0887096774 0.0978023093 -0.0887096774\\n0.0887096774 0.1029319833 -0.0532258065\\n0.0887096774 0.1044289205 -0.0177419355\\n0.0887096774 0.1050707955 0.0177419355\\n0.0887096774 0.1033044014 0.0532258065\\n0.0887096774 0.0937573616 0.0887096774\\n0.1241935484 -0.0736099867 -0.0887096774\\n0.1241935484 -0.0532258065 -0.1029521106\\n0.1241935484 -0.0706503588 -0.0532258065\\n0.1241935484 -0.0731573611 -0.0177419355\\n0.1241935484 -0.0724955328 0.0177419355\\n0.1241935484 -0.0704089302 0.0532258065\\n0.1241935484 -0.0733745198 0.0887096774\\n0.1241935484 -0.0532258065 0.1060931329\\n0.1241935484 -0.0177419355 -0.1055051077\\n0.1241935484 -0.0177419355 0.1072154355\\n0.1241935484 0.0177419355 -0.1136053322\\n0.1241935484 0.0177419355 0.1121946636\\n0.1224917229 0.0532258065 -0.0887096774\\n0.1241935484 0.0518088462 -0.0887096774\\n0.1235994835 0.0532258065 -0.0532258065\\n0.1241935484 0.0525711911 -0.0532258065\\n0.1241935484 0.0532258065 -0.0370441209\\n0.1228787391 0.0532258065 0.0532258065\\n0.1241935484 0.0516620052 0.0532258065\\n0.1241935484 0.0532258065 0.0343588670\\n0.1203826743 0.0532258065 0.0887096774\\n0.1241935484 0.0502945060 0.0887096774\\n0.0996561511 0.0532258065 0.1241935484\\n0.0952573709 0.0887096774 -0.0887096774\\n0.0987921195 0.0887096774 -0.0532258065\\n0.1014498753 0.0887096774 -0.0177419355\\n0.1241935484 0.0545464923 -0.0177419355\\n0.1021807052 0.0887096774 0.0177419355\\n0.1241935484 0.0555350687 0.0177419355\\n0.0993913394 0.0887096774 0.0532258065\\n0.0922815687 0.0887096774 0.0887096774\\n0.1414626377 -0.0887096774 -0.0887096774\\n0.1596774194 -0.1117945752 -0.0887096774\\n0.1596774194 -0.0887096774 -0.1069053580\\n0.1596774194 -0.0887096774 -0.0624675098\\n0.1413431304 -0.0887096774 0.0887096774\\n0.1596774194 -0.1121529538 0.0887096774\\n0.1596774194 -0.0887096774 0.0617755442\\n0.1596774194 -0.0887096774 0.1065657837\\n0.1596774194 -0.0532258065 -0.1060360531\\n0.1596774194 -0.0762783908 -0.0532258065\\n0.1596774194 -0.0781318129 -0.0177419355\\n0.1596774194 -0.0779947323 0.0177419355\\n0.1596774194 -0.0759359347 0.0532258065\\n0.1596774194 -0.0532258065 0.1081757553\\n0.1586848625 -0.0177419355 -0.0887096774\\n0.1596774194 -0.0197548268 -0.0887096774\\n0.1596774194 -0.0177419355 -0.0762574443\\n0.1587321109 -0.0177419355 0.0887096774\\n0.1596774194 -0.0196064559 0.0887096774\\n0.1596774194 -0.0177419355 0.0807306347\\n0.1596774194 -0.0167046548 -0.0887096774\\n0.1596774194 0.0177419355 -0.1107455573\\n0.1596774194 -0.0167647453 0.0887096774\\n0.1596774194 0.0177419355 0.1092645872\\n0.1596774194 0.0365849059 -0.0887096774\\n0.1596774194 0.0322683460 -0.0532258065\\n0.1250704874 0.0532258065 -0.0177419355\\n0.1596774194 0.0329897986 -0.0177419355\\n0.1257399344 0.0532258065 0.0177419355\\n0.1596774194 0.0337886078 0.0177419355\\n0.1596774194 0.0332544721 0.0532258065\\n0.1596774194 0.0369117172 0.0887096774\\n0.1842085577 -0.1241935484 -0.0887096774\\n0.1951612903 -0.1269498097 -0.0887096774\\n0.1951612903 -0.1241935484 -0.0933343934\\n0.1951612903 -0.1241935484 -0.0832144704\\n0.1829744290 -0.1241935484 0.0887096774\\n0.1951612903 -0.1274085267 0.0887096774\\n0.1951612903 -0.1241935484 0.0805682783\\n0.1951612903 -0.1241935484 0.0947865285\\n0.1951612903 -0.0887096774 -0.1009089474\\n0.1951612903 -0.0887096774 -0.0577581934\\n0.1928701142 -0.0887096774 -0.0177419355\\n0.1951612903 -0.0890820914 -0.0177419355\\n0.1951612903 -0.0887096774 -0.0258286092\\n0.1951449937 -0.0887096774 0.0177419355\\n0.1951612903 -0.0887121964 0.0177419355\\n0.1951612903 -0.0887096774 0.0177960690\\n0.1951612903 -0.0887096774 0.0585000857\\n0.1951612903 -0.0887096774 0.1016236624\\n0.1951612903 -0.0532258065 -0.1041554406\\n0.1951612903 -0.0845690964 -0.0532258065\\n0.1951612903 -0.0832397891 0.0532258065\\n0.1951612903 -0.0532258065 0.1058933063\\n0.1951612903 -0.0196044632 -0.0887096774\\n0.1951612903 -0.0177419355 -0.0833247124\\n0.1807142178 -0.0177419355 0.0887096774\\n0.1951612903 -0.0177419355 0.0898605242\\n0.1951612903 -0.0167850208 -0.0887096774\\n0.1951612903 0.0177419355 -0.1072995598\\n0.1951612903 0.0177419355 0.1098601837\\n0.1951612903 0.0310539212 -0.0887096774\\n0.1951612903 0.0317075469 -0.0532258065\\n0.1951612903 0.0320150772 -0.0177419355\\n0.1951612903 0.0324679000 0.0177419355\\n0.1951612903 0.0323015413 0.0532258065\\n0.1951612903 0.0322249493 0.0887096774\\n0.1998272293 -0.1241935484 -0.0887096774\\n0.2007018717 -0.1241935484 0.0887096774\\n0.2306451613 -0.1004869841 -0.0887096774\\n0.2306451613 -0.0887096774 -0.1074201601\\n0.2306451613 -0.0887096774 -0.0701100995\\n0.1967264501 -0.0887096774 -0.0177419355\\n0.1951719123 -0.0887096774 0.0177419355\\n0.2306451613 -0.1000316766 0.0887096774\\n0.2306451613 -0.0887096774 0.0699054749\\n0.2306451613 -0.0887096774 0.1054592071\\n0.2306451613 -0.0532258065 -0.0987757026\\n0.2306451613 -0.0695930967 -0.0532258065\\n0.2306451613 -0.0769824607 -0.0177419355\\n0.2306451613 -0.0765241877 0.0177419355\\n0.2306451613 -0.0675090832 0.0532258065\\n0.2306451613 -0.0532258065 0.0967316845\\n0.1972446469 -0.0177419355 -0.0887096774\\n0.2306451613 -0.0177419355 -0.1001336727\\n0.2306451613 -0.0177419355 0.1011523297\\n0.2306451613 0.0177419355 -0.0993917859\\n0.2306451613 0.0177419355 0.1013671254\\n0.2306451613 0.0237051709 -0.0887096774\\n0.2306451613 0.0275650132 -0.0532258065\\n0.2306451613 0.0277905165 -0.0177419355\\n0.2306451613 0.0279664292 0.0177419355\\n0.2306451613 0.0280639142 0.0532258065\\n0.2306451613 0.0244284827 0.0887096774\\n0.2388273912 -0.0887096774 -0.0887096774\\n0.2392146091 -0.0887096774 0.0887096774\\n0.2491017279 -0.0532258065 -0.0887096774\\n0.2661290323 -0.0557304483 -0.0532258065\\n0.2661290323 -0.0532258065 -0.0646833130\\n0.2661290323 -0.0583686602 -0.0177419355\\n0.2661290323 -0.0583671707 0.0177419355\\n0.2661290323 -0.0564153101 0.0532258065\\n0.2452764604 -0.0532258065 0.0887096774\\n0.2661290323 -0.0532258065 0.0668741609\\n0.2529015991 -0.0177419355 -0.0887096774\\n0.2661290323 -0.0177419355 -0.0709924591\\n0.2564844829 -0.0177419355 0.0887096774\\n0.2661290323 -0.0177419355 0.0743462249\\n0.2439981839 0.0177419355 -0.0887096774\\n0.2552660870 0.0177419355 -0.0532258065\\n0.2661290323 -0.0030732785 -0.0532258065\\n0.2597217339 0.0177419355 -0.0177419355\\n0.2661290323 0.0027008520 -0.0177419355\\n0.2598290734 0.0177419355 0.0177419355\\n0.2661290323 0.0027239748 0.0177419355\\n0.2563261448 0.0177419355 0.0532258065\\n0.2661290323 -0.0032059816 0.0532258065\\n0.2455653363 0.0177419355 0.0887096774\\n0.2665960341 -0.0532258065 -0.0532258065\\n0.2669891895 -0.0532258065 -0.0177419355\\n0.2669959232 -0.0532258065 0.0177419355\\n0.2666925448 -0.0532258065 0.0532258065\\n0.2671659195 -0.0177419355 -0.0532258065\\n0.2672341995 -0.0177419355 -0.0177419355\\n0.2672579483 -0.0177419355 0.0177419355\\n0.2670770885 -0.0177419355 0.0532258065\\n3 2 1 0\\n3 1 4 3\\n3 0 1 3\\n3 4 6 5\\n3 3 4 5\\n3 6 8 7\\n3 5 6 7\\n3 8 10 9\\n3 7 8 9\\n3 10 12 11\\n3 9 10 11\\n3 12 13 11\\n3 2 0 14\\n3 0 3 15\\n3 14 0 15\\n3 17 15 3\\n3 17 3 16\\n3 16 3 5\\n3 18 19 16\\n3 18 16 7\\n3 7 16 5\\n3 7 9 20\\n3 18 7 20\\n3 9 11 21\\n3 20 9 21\\n3 11 13 21\\n3 17 16 22\\n3 16 19 22\\n3 2 24 23\\n3 1 2 23\\n3 23 25 1\\n3 1 25 4\\n3 25 26 4\\n3 4 26 6\\n3 26 27 6\\n3 6 27 8\\n3 27 28 8\\n3 8 28 10\\n3 28 29 10\\n3 10 29 12\\n3 12 29 30\\n3 13 12 30\\n3 14 31 32\\n3 14 32 2\\n3 2 32 24\\n3 15 31 14\\n3 33 31 15\\n3 15 17 33\\n3 18 34 19\\n3 20 34 18\\n3 35 34 20\\n3 21 35 20\\n3 36 35 21\\n3 36 21 13\\n3 36 13 37\\n3 37 13 30\\n3 39 32 31\\n3 38 39 31\\n3 31 33 38\\n3 33 40 38\\n3 22 33 17\\n3 22 41 33\\n3 41 40 33\\n3 19 41 22\\n3 19 34 41\\n3 34 42 41\\n3 34 35 42\\n3 35 43 42\\n3 35 36 43\\n3 36 44 43\\n3 37 45 44\\n3 36 37 44\\n3 47 39 38\\n3 46 47 38\\n3 38 40 46\\n3 40 48 46\\n3 40 41 48\\n3 41 49 48\\n3 41 42 49\\n3 42 50 49\\n3 42 43 50\\n3 43 51 50\\n3 43 44 51\\n3 44 52 51\\n3 45 53 52\\n3 44 45 52\\n3 55 47 46\\n3 54 55 46\\n3 46 48 54\\n3 48 56 54\\n3 48 49 56\\n3 49 57 56\\n3 49 50 57\\n3 50 58 57\\n3 50 51 58\\n3 51 59 58\\n3 51 52 59\\n3 52 60 59\\n3 53 61 60\\n3 52 53 60\\n3 55 54 62\\n3 54 56 63\\n3 62 54 63\\n3 56 57 64\\n3 63 56 64\\n3 57 58 65\\n3 64 57 65\\n3 58 59 66\\n3 65 58 66\\n3 59 60 67\\n3 66 59 67\\n3 60 61 67\\n3 70 69 68\\n3 69 71 68\\n3 74 73 72\\n3 73 75 72\\n3 68 23 24\\n3 68 24 70\\n3 70 24 76\\n3 71 23 68\\n3 71 77 23\\n3 77 25 23\\n3 79 78 26\\n3 79 26 77\\n3 77 26 25\\n3 27 26 78\\n3 80 27 78\\n3 28 27 80\\n3 81 28 80\\n3 81 82 74\\n3 81 74 29\\n3 81 29 28\\n3 29 74 72\\n3 29 72 75\\n3 29 75 30\\n3 30 75 83\\n3 76 24 84\\n3 84 24 32\\n3 79 85 78\\n3 78 86 80\\n3 85 86 78\\n3 80 87 81\\n3 86 87 80\\n3 81 87 82\\n3 83 88 30\\n3 88 37 30\\n3 84 32 89\\n3 89 32 39\\n3 88 90 37\\n3 90 45 37\\n3 89 39 91\\n3 91 39 47\\n3 90 92 45\\n3 92 53 45\\n3 91 47 93\\n3 93 47 55\\n3 92 94 53\\n3 94 61 53\\n3 93 55 62\\n3 95 93 62\\n3 95 62 96\\n3 62 63 96\\n3 96 63 97\\n3 63 64 97\\n3 97 64 98\\n3 64 65 98\\n3 98 65 99\\n3 65 66 99\\n3 99 66 100\\n3 66 67 100\\n3 100 67 61\\n3 94 100 61\\n3 103 102 101\\n3 102 104 101\\n3 107 106 105\\n3 106 108 105\\n3 101 69 70\\n3 101 70 103\\n3 103 70 109\\n3 69 101 104\\n3 69 104 71\\n3 71 104 110\\n3 113 112 111\\n3 112 115 114\\n3 111 112 114\\n3 115 116 114\\n3 105 73 74\\n3 105 74 107\\n3 107 74 117\\n3 73 105 108\\n3 73 108 75\\n3 75 108 118\\n3 109 70 119\\n3 119 70 76\\n3 110 77 71\\n3 120 77 110\\n3 113 111 121\\n3 113 121 77\\n3 113 77 120\\n3 79 77 121\\n3 111 114 121\\n3 114 122 121\\n3 116 124 123\\n3 116 123 114\\n3 114 123 122\\n3 124 82 123\\n3 124 117 82\\n3 117 74 82\\n3 118 125 75\\n3 125 83 75\\n3 119 76 126\\n3 126 76 84\\n3 121 85 79\\n3 122 85 121\\n3 86 85 122\\n3 123 86 122\\n3 87 86 123\\n3 123 82 87\\n3 125 127 83\\n3 127 88 83\\n3 126 84 128\\n3 128 84 89\\n3 127 129 88\\n3 129 90 88\\n3 128 89 130\\n3 130 89 91\\n3 129 131 90\\n3 131 92 90\\n3 130 91 132\\n3 132 91 93\\n3 131 133 92\\n3 133 94 92\\n3 132 93 95\\n3 134 132 95\\n3 134 95 135\\n3 95 96 135\\n3 135 96 136\\n3 96 97 136\\n3 136 97 137\\n3 97 98 137\\n3 137 98 138\\n3 98 99 138\\n3 138 99 139\\n3 99 100 139\\n3 139 100 94\\n3 133 139 94\\n3 102 103 140\\n3 102 140 104\\n3 106 107 141\\n3 106 141 108\\n3 142 140 103\\n3 142 103 143\\n3 143 103 109\\n3 140 142 144\\n3 140 144 104\\n3 104 144 110\\n3 112 113 145\\n3 115 112 145\\n3 146 115 145\\n3 115 146 116\\n3 147 141 107\\n3 147 107 148\\n3 148 107 117\\n3 141 147 149\\n3 141 149 108\\n3 108 149 118\\n3 143 109 150\\n3 150 109 119\\n3 144 120 110\\n3 151 120 144\\n3 113 120 151\\n3 113 151 152\\n3 113 152 145\\n3 152 151 153\\n3 145 152 146\\n3 146 152 154\\n3 116 146 154\\n3 116 154 155\\n3 116 155 124\\n3 156 155 154\\n3 155 117 124\\n3 148 117 155\\n3 149 157 118\\n3 157 125 118\\n3 150 119 158\\n3 158 119 126\\n3 153 159 152\\n3 152 160 154\\n3 159 160 152\\n3 154 160 156\\n3 157 161 125\\n3 161 127 125\\n3 158 126 162\\n3 162 126 128\\n3 161 163 127\\n3 163 129 127\\n3 162 128 164\\n3 164 128 130\\n3 163 165 129\\n3 165 131 129\\n3 164 130 166\\n3 166 130 132\\n3 165 167 131\\n3 167 133 131\\n3 166 132 134\\n3 168 166 134\\n3 168 134 169\\n3 134 135 169\\n3 169 135 170\\n3 135 136 170\\n3 170 136 171\\n3 136 137 171\\n3 171 137 172\\n3 137 138 172\\n3 172 138 173\\n3 138 139 173\\n3 173 139 133\\n3 167 173 133\\n3 142 143 174\\n3 142 174 144\\n3 147 148 175\\n3 147 175 149\\n3 176 174 143\\n3 176 143 177\\n3 177 143 150\\n3 174 151 144\\n3 174 176 151\\n3 176 178 151\\n3 179 153 151\\n3 179 151 180\\n3 180 151 178\\n3 180 182 181\\n3 179 180 181\\n3 156 181 182\\n3 156 182 155\\n3 155 182 183\\n3 148 184 175\\n3 148 155 184\\n3 155 183 184\\n3 175 184 185\\n3 175 185 149\\n3 149 185 157\\n3 177 150 186\\n3 186 150 158\\n3 179 159 153\\n3 181 159 179\\n3 160 159 181\\n3 181 156 160\\n3 185 187 157\\n3 187 161 157\\n3 186 158 188\\n3 188 158 162\\n3 187 189 161\\n3 189 163 161\\n3 188 162 190\\n3 190 162 164\\n3 189 191 163\\n3 191 165 163\\n3 190 164 192\\n3 192 164 166\\n3 191 193 165\\n3 193 167 165\\n3 192 166 168\\n3 194 192 168\\n3 194 168 195\\n3 168 169 195\\n3 195 169 196\\n3 169 170 196\\n3 196 170 197\\n3 170 171 197\\n3 197 171 198\\n3 171 172 198\\n3 198 172 199\\n3 172 173 199\\n3 199 173 167\\n3 193 199 167\\n3 177 201 200\\n3 176 177 200\\n3 200 202 176\\n3 176 202 178\\n3 202 203 178\\n3 178 203 180\\n3 203 204 180\\n3 180 204 182\\n3 204 205 182\\n3 182 205 183\\n3 205 206 183\\n3 183 206 184\\n3 184 206 207\\n3 185 184 207\\n3 201 177 208\\n3 208 177 186\\n3 207 209 185\\n3 209 187 185\\n3 208 186 210\\n3 210 186 188\\n3 209 211 187\\n3 211 189 187\\n3 210 188 212\\n3 212 188 190\\n3 211 213 189\\n3 213 191 189\\n3 212 190 214\\n3 214 190 192\\n3 213 215 191\\n3 215 193 191\\n3 214 192 194\\n3 216 214 194\\n3 216 194 217\\n3 194 195 217\\n3 217 195 218\\n3 195 196 218\\n3 218 196 219\\n3 196 197 219\\n3 219 197 220\\n3 197 198 220\\n3 220 198 221\\n3 198 199 221\\n3 221 199 193\\n3 215 221 193\\n3 201 223 222\\n3 200 201 222\\n3 222 224 200\\n3 200 224 202\\n3 224 225 202\\n3 202 225 203\\n3 225 226 203\\n3 203 226 204\\n3 226 227 204\\n3 204 227 205\\n3 227 228 205\\n3 205 228 206\\n3 206 228 229\\n3 207 206 229\\n3 223 201 230\\n3 230 201 208\\n3 229 231 207\\n3 231 209 207\\n3 230 208 232\\n3 232 208 210\\n3 231 233 209\\n3 233 211 209\\n3 232 210 234\\n3 234 210 212\\n3 233 235 211\\n3 235 213 211\\n3 234 212 236\\n3 236 212 214\\n3 235 237 213\\n3 237 215 213\\n3 236 214 216\\n3 238 236 216\\n3 238 216 239\\n3 216 217 239\\n3 239 217 240\\n3 217 218 240\\n3 240 218 241\\n3 218 219 241\\n3 241 219 242\\n3 219 220 242\\n3 242 220 243\\n3 220 221 243\\n3 243 221 215\\n3 237 243 215\\n3 223 245 244\\n3 222 223 244\\n3 244 246 222\\n3 222 246 224\\n3 246 247 224\\n3 224 247 225\\n3 247 248 225\\n3 225 248 226\\n3 248 249 226\\n3 226 249 227\\n3 249 250 227\\n3 227 250 228\\n3 228 250 251\\n3 229 228 251\\n3 245 223 252\\n3 252 223 230\\n3 251 253 229\\n3 253 231 229\\n3 252 230 254\\n3 254 230 232\\n3 253 255 231\\n3 255 233 231\\n3 254 232 256\\n3 256 232 234\\n3 255 257 233\\n3 257 235 233\\n3 256 234 258\\n3 258 234 236\\n3 257 259 235\\n3 259 237 235\\n3 258 236 238\\n3 260 258 238\\n3 260 238 261\\n3 238 239 261\\n3 261 239 262\\n3 239 240 262\\n3 262 240 263\\n3 240 241 263\\n3 263 241 264\\n3 241 242 264\\n3 264 242 265\\n3 242 243 265\\n3 265 243 237\\n3 259 265 237\\n3 245 267 266\\n3 244 245 266\\n3 266 268 244\\n3 244 268 246\\n3 268 269 246\\n3 246 269 247\\n3 269 270 247\\n3 247 270 248\\n3 270 271 248\\n3 248 271 249\\n3 271 272 249\\n3 249 272 250\\n3 250 272 273\\n3 251 250 273\\n3 267 245 274\\n3 274 245 252\\n3 273 275 251\\n3 275 253 251\\n3 274 252 276\\n3 276 252 254\\n3 275 277 253\\n3 277 255 253\\n3 276 254 278\\n3 278 254 256\\n3 277 279 255\\n3 279 257 255\\n3 278 256 280\\n3 280 256 258\\n3 279 281 257\\n3 281 259 257\\n3 280 258 260\\n3 282 280 260\\n3 282 260 283\\n3 260 261 283\\n3 283 261 284\\n3 261 262 284\\n3 284 262 285\\n3 262 263 285\\n3 285 263 286\\n3 263 264 286\\n3 286 264 287\\n3 264 265 287\\n3 287 265 259\\n3 281 287 259\\n3 267 289 288\\n3 266 267 288\\n3 288 290 266\\n3 266 290 268\\n3 290 291 268\\n3 268 291 269\\n3 291 292 269\\n3 269 292 270\\n3 292 293 270\\n3 270 293 271\\n3 293 294 271\\n3 271 294 272\\n3 272 294 295\\n3 273 272 295\\n3 289 267 296\\n3 296 267 274\\n3 295 297 273\\n3 297 275 273\\n3 296 274 298\\n3 298 274 276\\n3 297 299 275\\n3 299 277 275\\n3 298 276 300\\n3 300 276 278\\n3 301 299 302\\n3 301 279 299\\n3 279 277 299\\n3 302 303 301\\n3 300 278 304\\n3 304 278 280\\n3 305 279 301\\n3 305 306 279\\n3 306 281 279\\n3 301 303 305\\n3 304 280 282\\n3 307 304 282\\n3 307 282 308\\n3 282 283 308\\n3 308 283 309\\n3 283 284 309\\n3 309 284 310\\n3 284 285 310\\n3 310 285 311\\n3 285 286 311\\n3 311 286 312\\n3 286 287 312\\n3 312 287 281\\n3 306 312 281\\n3 289 314 313\\n3 288 289 313\\n3 313 315 288\\n3 288 315 290\\n3 315 316 290\\n3 290 316 291\\n3 316 317 291\\n3 291 317 292\\n3 317 318 292\\n3 292 318 293\\n3 318 319 293\\n3 293 319 294\\n3 294 319 320\\n3 295 294 320\\n3 314 289 321\\n3 321 289 296\\n3 320 322 295\\n3 322 297 295\\n3 321 296 323\\n3 323 296 298\\n3 322 324 297\\n3 324 299 297\\n3 325 326 323\\n3 325 323 300\\n3 300 323 298\\n3 326 327 328\\n3 325 327 326\\n3 328 327 329\\n3 332 330 331\\n3 331 333 334\\n3 330 333 331\\n3 334 333 335\\n3 334 335 299\\n3 334 299 324\\n3 299 335 302\\n3 302 335 303\\n3 336 325 300\\n3 304 336 300\\n3 325 336 327\\n3 327 336 337\\n3 329 338 339\\n3 329 327 338\\n3 327 337 338\\n3 339 340 341\\n3 338 340 339\\n3 341 330 332\\n3 341 340 330\\n3 340 342 330\\n3 330 342 333\\n3 333 342 343\\n3 306 305 335\\n3 306 335 343\\n3 343 335 333\\n3 335 305 303\\n3 336 304 307\\n3 337 336 307\\n3 308 337 307\\n3 338 337 308\\n3 309 338 308\\n3 340 338 309\\n3 310 340 309\\n3 342 340 310\\n3 311 342 310\\n3 343 342 311\\n3 312 343 311\\n3 343 312 306\\n3 346 345 344\\n3 345 347 344\\n3 350 349 348\\n3 349 351 348\\n3 344 313 314\\n3 344 314 346\\n3 346 314 352\\n3 347 313 344\\n3 347 353 313\\n3 353 315 313\\n3 353 354 315\\n3 315 354 316\\n3 354 355 316\\n3 316 355 317\\n3 355 356 317\\n3 317 356 318\\n3 348 356 350\\n3 348 319 356\\n3 319 318 356\\n3 319 348 351\\n3 319 351 320\\n3 320 351 357\\n3 358 359 352\\n3 358 352 321\\n3 321 352 314\\n3 359 358 360\\n3 363 361 362\\n3 362 361 322\\n3 362 322 357\\n3 357 322 320\\n3 364 358 321\\n3 364 321 365\\n3 365 321 323\\n3 358 364 360\\n3 363 366 361\\n3 361 366 367\\n3 361 367 322\\n3 322 367 324\\n3 365 323 326\\n3 368 365 326\\n3 368 326 369\\n3 326 328 369\\n3 370 328 329\\n3 370 371 328\\n3 371 369 328\\n3 371 372 373\\n3 370 372 371\\n3 332 373 372\\n3 332 331 373\\n3 331 374 373\\n3 374 331 375\\n3 331 334 375\\n3 375 334 324\\n3 367 375 324\\n3 370 329 339\\n3 372 370 339\\n3 341 372 339\\n3 372 341 332\\n3 378 377 376\\n3 377 379 376\\n3 382 381 380\\n3 381 383 380\\n3 376 345 346\\n3 376 346 378\\n3 378 346 384\\n3 345 376 379\\n3 345 379 347\\n3 347 379 385\\n3 388 387 386\\n3 387 390 389\\n3 386 387 389\\n3 390 391 389\\n3 380 349 350\\n3 380 350 382\\n3 382 350 392\\n3 349 380 383\\n3 349 383 351\\n3 351 383 393\\n3 384 346 394\\n3 394 346 352\\n3 385 353 347\\n3 395 353 385\\n3 386 395 388\\n3 386 354 395\\n3 354 353 395\\n3 389 354 386\\n3 355 354 389\\n3 391 355 389\\n3 391 396 355\\n3 396 356 355\\n3 396 350 356\\n3 392 350 396\\n3 393 397 351\\n3 397 357 351\\n3 394 352 359\\n3 398 394 359\\n3 359 399 398\\n3 360 399 359\\n3 362 400 363\\n3 400 362 357\\n3 400 357 401\\n3 401 357 397\\n3 365 403 402\\n3 364 365 402\\n3 402 360 364\\n3 399 360 402\\n3 400 366 363\\n3 366 400 401\\n3 366 401 367\\n3 367 401 404\\n3 403 365 368\\n3 405 403 368\\n3 405 368 406\\n3 368 369 406\\n3 406 369 407\\n3 369 371 407\\n3 407 371 408\\n3 371 373 408\\n3 408 373 409\\n3 373 374 409\\n3 409 374 410\\n3 374 375 410\\n3 410 375 367\\n3 404 410 367\\n3 377 378 411\\n3 377 411 379\\n3 381 382 412\\n3 381 412 383\\n3 413 411 378\\n3 413 378 414\\n3 414 378 384\\n3 411 413 415\\n3 411 415 379\\n3 379 415 385\\n3 387 388 416\\n3 390 387 416\\n3 417 390 416\\n3 390 417 391\\n3 418 412 382\\n3 418 382 419\\n3 419 382 392\\n3 412 418 420\\n3 412 420 383\\n3 383 420 393\\n3 414 384 421\\n3 421 384 394\\n3 415 395 385\\n3 422 395 415\\n3 388 423 416\\n3 388 395 423\\n3 395 422 423\\n3 416 424 417\\n3 423 424 416\\n3 417 396 391\\n3 417 424 396\\n3 424 425 396\\n3 425 392 396\\n3 419 392 425\\n3 420 426 393\\n3 426 397 393\\n3 398 427 428\\n3 398 428 394\\n3 394 428 421\\n3 398 399 427\\n3 426 429 397\\n3 429 401 397\\n3 427 402 403\\n3 427 403 428\\n3 428 403 430\\n3 427 399 402\\n3 429 431 401\\n3 431 404 401\\n3 430 403 405\\n3 432 430 405\\n3 432 405 433\\n3 405 406 433\\n3 433 406 434\\n3 406 407 434\\n3 434 407 435\\n3 407 408 435\\n3 435 408 436\\n3 408 409 436\\n3 436 409 437\\n3 409 410 437\\n3 437 410 404\\n3 431 437 404\\n3 413 414 438\\n3 413 438 415\\n3 418 419 439\\n3 418 439 420\\n3 440 438 414\\n3 421 440 414\\n3 415 438 440\\n3 415 440 441\\n3 415 441 422\\n3 442 441 440\\n3 441 443 422\\n3 422 443 423\\n3 443 444 423\\n3 423 444 424\\n3 444 445 424\\n3 424 445 425\\n3 419 425 445\\n3 419 445 446\\n3 419 446 439\\n3 446 445 447\\n3 439 446 426\\n3 420 439 426\\n3 448 440 421\\n3 428 448 421\\n3 442 448 449\\n3 440 448 442\\n3 451 446 447\\n3 450 446 451\\n3 446 450 429\\n3 426 446 429\\n3 452 448 428\\n3 430 452 428\\n3 449 453 454\\n3 449 448 453\\n3 448 452 453\\n3 454 455 456\\n3 453 455 454\\n3 456 457 458\\n3 455 457 456\\n3 458 459 460\\n3 457 459 458\\n3 460 450 451\\n3 460 459 450\\n3 459 461 450\\n3 450 461 431\\n3 429 450 431\\n3 452 430 432\\n3 453 452 432\\n3 433 453 432\\n3 455 453 433\\n3 434 455 433\\n3 457 455 434\\n3 435 457 434\\n3 459 457 435\\n3 436 459 435\\n3 461 459 436\\n3 437 461 436\\n3 461 437 431\\n3 441 442 462\\n3 443 441 462\\n3 463 443 462\\n3 444 443 463\\n3 464 444 463\\n3 445 444 464\\n3 465 445 464\\n3 445 465 447\\n3 466 462 442\\n3 449 466 442\\n3 462 466 463\\n3 463 466 467\\n3 463 467 464\\n3 464 467 468\\n3 464 468 465\\n3 465 468 469\\n3 465 469 451\\n3 447 465 451\\n3 466 449 454\\n3 467 466 454\\n3 456 467 454\\n3 468 467 456\\n3 458 468 456\\n3 469 468 458\\n3 460 469 458\\n3 469 460 451'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ_file = '../project-noisypixel/sample_data/points/occupancies.npy'\n",
    "points_file = '../project-noisypixel/sample_data/points/points.npy'\n",
    "\n",
    "points = np.load(points_file)\n",
    "\n",
    "occ = np.load(occ_file)\n",
    "occ = np.unpackbits(occ)\n",
    "\n",
    "# idx = np.random.choice(np.arange(100000), 32768, replace=False)\n",
    "# occ_sample = occ[idx]\n",
    "# points_sample = points[idx]\n",
    "\n",
    "\n",
    "\n",
    "mesh = get_mesh(occ_sample,points_sample)\n",
    "\n",
    "\n",
    "mesh_out_file = os.path.join('./', '%s.off' % 'onet')\n",
    "mesh.export(mesh_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
